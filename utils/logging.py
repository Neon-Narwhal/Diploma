"""–°–∏—Å—Ç–µ–º–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"""

import torch
import logging
from typing import List, Optional
from datetime import datetime


class GenerationLogger:
    """–ö–ª–∞—Å—Å –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"""

    def __init__(self, log_level: str = "INFO"):
        self.logger = logging.getLogger("GenerationLogger")
        self.logger.setLevel(getattr(logging, log_level))

        # –°–æ–∑–¥–∞–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–Ω—Å–æ–ª–∏ –µ—Å–ª–∏ –æ–Ω –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)

    def log_generation_start(self, context: torch.Tensor, max_chars: int):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        self.logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞")
        self.logger.info(f"   –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context.shape}")
        self.logger.info(f"   –ú–∞–∫—Å–∏–º—É–º —Å–∏–º–≤–æ–ª–æ–≤: {max_chars}")
        self.logger.info(f"   –í—Ä–µ–º—è: {datetime.now().strftime('%H:%M:%S')}")

    def log_generation_step(self, step: int, token_id: int, decoded_token: str):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
        display_token = repr(decoded_token) if decoded_token in ['\n', '\t', ' '] else decoded_token
        self.logger.debug(f"   –®–∞–≥ {step:3d}: token_id={token_id:4d} -> '{display_token}'")

    def log_generation_complete(self, generated_text: str, total_tokens: int, execution_time: float):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        self.logger.info(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        self.logger.info(f"   –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens}")
        self.logger.info(f"   –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time:.2f}—Å")
        self.logger.info(f"   –°–∏–º–≤–æ–ª–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É: {len(generated_text)/execution_time:.1f}")
        self.logger.info(f"\nüìù –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:")
        self.logger.info(f"{'='*50}")
        self.logger.info(generated_text)
        self.logger.info(f"{'='*50}")

    def log_tokenizer_info(self, tokenizer_type: str, vocab_size: int):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–µ"""
        self.logger.info(f"üî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {tokenizer_type.upper()}")
        self.logger.info(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size}")

    def log_model_info(self, model, device: str):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        param_count = sum(p.numel() for p in model.parameters())
        self.logger.info(f"üß† –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        self.logger.info(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {param_count:,}")
        self.logger.info(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
