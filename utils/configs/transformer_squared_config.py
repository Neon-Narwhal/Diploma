# =============================================================================
# utils/configs/transformer_squared_config.py
# –ü–û–õ–ù–ê–Ø –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Transformer¬≤ - –≤—Å–µ –ø–æ–ª—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –º–æ–¥–µ–ª—å—é
# =============================================================================

from dataclasses import dataclass, field
from typing import Optional, List
import torch
from utils.configs.base_config import BaseTrainingConfig


@dataclass
class TransformerSquaredConfig(BaseTrainingConfig):
    """
    –ü–û–õ–ù–ê–Ø –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è Transformer¬≤ –º–æ–¥–µ–ª–∏.
    –í–∫–ª—é—á–∞–µ—Ç –í–°–ï –ø–æ–ª—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏.
    """
    
    # ============================================================================
    # –ë–ê–ó–û–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê
    # ============================================================================
    
    n_embd: int = 256
    """–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."""
    
    n_layer: int = 6
    """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ transformer layers."""
    
    n_head: int = 4
    """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ attention heads."""
    
    n_inner: Optional[int] = None
    """FFN hidden size. –ï—Å–ª–∏ None, = 4 * n_embd."""

    ffn_expansion_factor: int = 4
    """–ú–Ω–æ–∂–∏—Ç–µ–ª—å –¥–ª—è FFN —Ä–∞–∑–º–µ—Ä–∞ (hidden_dim = n_embd * ffn_expansion_factor).
    –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: 4 (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º Transformer)."""
    
    # ============================================================================
    # SVF (Singular Value Fine-tuning) –ü–ê–†–ê–ú–ï–¢–†–´
    # ============================================================================
    
    use_svf: bool = True
    """–í–∫–ª—é—á–∏—Ç—å SVF –º–µ—Ö–∞–Ω–∏–∑–º."""
    
    svf_rank: int = 8
    """–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–Ω–≥ SVF (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)."""
    
    svf_rank_ratio: float = 0.25
    """–û—Ç–Ω–æ—à–µ–Ω–∏–µ rank –∫ —Ä–∞–∑–º–µ—Ä—É —Å–ª–æ—è (rank = min(in, out) * ratio).
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ—Å–ª–∏ svf_rank –Ω–µ –∑–∞–¥–∞–Ω —è–≤–Ω–æ."""
    
    svf_alpha: float = 16.0
    """Scaling —Ñ–∞–∫—Ç–æ—Ä –¥–ª—è SVF –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π."""
    
    svf_dropout: float = 0.1
    """Dropout –¥–ª—è SVF."""
    
    svf_bias: bool = False
    """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å bias –≤ SVF —Å–ª–æ—è—Ö."""
    
    apply_svf_to: List[str] = field(default_factory=lambda: ['q', 'v'])
    """–ö –∫–∞–∫–∏–º –º–∞—Ç—Ä–∏—Ü–∞–º –ø—Ä–∏–º–µ–Ω—è—Ç—å SVF: 'q', 'k', 'v', 'o', 'ffn'."""
    
    svf_initializer_range: float = 0.01
    """Std –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ SVF –≤–µ—Å–æ–≤."""
    
    # ============================================================================
    # EXPERT VECTORS
    # ============================================================================
    
    num_experts: int = 4
    """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ task-specific expert vectors."""
    
    expert_dim: int = 64
    """–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ expert vector."""
    
    expert_dropout: float = 0.1
    """Dropout –¥–ª—è expert vectors."""

    expert_vector_init_std: float = 0.01 
    """Standard deviation –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ expert vectors.
    –û–±—ã—á–Ω–æ –º–µ–Ω—å—à–µ —á–µ–º –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –≤–µ—Å–æ–≤ (0.01 vs 0.02)."""
    
    # ============================================================================
    # ADAPTATION STRATEGY
    # ============================================================================
    
    adaptation_strategy: str = 'mixture'
    """–°—Ç—Ä–∞—Ç–µ–≥–∏—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏: 'prompt', 'classifier', 'mixture'."""
    
    mixture_temperature: float = 1.0
    """–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è softmax –ø—Ä–∏ mixture strategy."""
    
    enable_expert_composition: bool = True
    """–ü–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å expert vectors."""
    
    composition_dropout: float = 0.05
    """Dropout –ø—Ä–∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ experts."""

    enable_task_detector: bool = False  # üÜï –î–û–ë–ê–í–¨–¢–ï –≠–¢–û
    """–í–∫–ª—é—á–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π task detector –¥–ª—è –≤—ã–±–æ—Ä–∞ expert vectors.
    –ï—Å–ª–∏ True, –º–æ–¥–µ–ª—å —Å–∞–º–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∑–∞–¥–∞—á–∏ –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤.
    –ï—Å–ª–∏ False, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è adaptation_strategy."""

    task_detector_hidden_dim: int = 128
    """–†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è –≤ task detector."""
    
    # ============================================================================
    # ROTARY POSITION EMBEDDING (RoPE)
    # ============================================================================
    
    use_rope: bool = True
    """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RoPE –≤–º–µ—Å—Ç–æ learned positional embeddings."""
    
    rope_theta: float = 10000.0
    """Base theta –¥–ª—è RoPE."""
    
    rope_scaling: Optional[float] = None
    """–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ RoPE –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤."""
    
    # ============================================================================
    # ADAPTIVE ATTENTION
    # ============================================================================
    
    use_adaptive_attention: bool = True
    """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å adaptive attention span."""
    
    adaptive_span_enabled: bool = False
    """–ü–æ–ª–Ω—ã–π adaptive span –º–µ—Ö–∞–Ω–∏–∑–º."""
    
    max_adaptive_span: int = 1024
    """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π attention span."""
    
    min_adaptive_span: int = 32
    """–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π attention span."""
    
    # ============================================================================
    # DROPOUT –ü–ê–†–ê–ú–ï–¢–†–´
    # ============================================================================
    
    attention_dropout: float = 0.1
    """Dropout –≤ attention."""
    
    residual_dropout: float = 0.1
    """Dropout –≤ residual connections."""
    
    embedding_dropout: float = 0.1
    """Dropout –≤ embeddings."""
    
    hidden_dropout: float = 0.1
    """Dropout –≤ hidden layers."""
    
    # ============================================================================
    # BIAS –ü–ê–†–ê–ú–ï–¢–†–´
    # ============================================================================
    
    use_bias: bool = True
    """–ì–ª–æ–±–∞–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è bias (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ—Å–ª–∏ specific –Ω–µ –∑–∞–¥–∞–Ω)."""
    
    attention_bias: bool = False
    """Bias –≤ attention –ø—Ä–æ–µ–∫—Ü–∏—è—Ö."""
    
    mlp_bias: bool = False
    """Bias –≤ MLP —Å–ª–æ—è—Ö."""
    
    embedding_bias: bool = False
    """Bias –≤ embeddings."""
    
    # ============================================================================
    # –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø
    # ============================================================================
    
    initializer_range: float = 0.02
    """Std –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö –≤–µ—Å–æ–≤."""

    weight_init_std: float = 0.02 
    """Standard deviation –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ _init_weights)."""

    embedding_init_std: float = 0.02  
    """Standard deviation –¥–ª—è embeddings –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏."""

    layernorm_init_std: float = 1.0  
    """Standard deviation –¥–ª—è LayerNorm –≤–µ—Å–æ–≤ (–æ–±—ã—á–Ω–æ 1.0)."""
    
    # ============================================================================
    # LAYER NORMALIZATION
    # ============================================================================
    
    layer_norm_epsilon: float = 1e-5
    """Epsilon –¥–ª—è layer normalization."""
    
    layer_norm_type: str = 'layernorm'
    """–¢–∏–ø normalization: 'layernorm', 'rmsnorm'."""

    use_pre_norm: bool = True  # üÜï –î–û–ë–ê–í–¨–¢–ï –≠–¢–û!
    """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Pre-LN (Layer Norm –ø–µ—Ä–µ–¥ sublayer) –≤–º–µ—Å—Ç–æ Post-LN.
    Pre-LN: –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (GPT-2, —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏)
    Post-LN: –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π Transformer
    –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: True (Pre-LN)"""
    
    # ============================================================================
    # ACTIVATION
    # ============================================================================
    
    activation_function: str = "gelu"
    """Activation —Ñ—É–Ω–∫—Ü–∏—è: 'gelu', 'relu', 'silu', 'swish'."""
    
    # ============================================================================
    # DTYPE
    # ============================================================================
    
    model_dtype: str = 'float32'
    """–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–∏: 'float32', 'float16', 'bfloat16'."""
    
    # ============================================================================
    # MEMORY OPTIMIZATION
    # ============================================================================
    
    use_gradient_checkpointing: bool = False
    """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å gradient checkpointing –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏."""
    
    use_flash_attention: bool = False
    """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Flash Attention (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)."""
    
    # ============================================================================
    # REINFORCEMENT LEARNING (–¥–ª—è –æ–±—É—á–µ–Ω–∏—è experts)
    # ============================================================================
    
    use_rl_for_experts: bool = False
    """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RL –¥–ª—è –æ–±—É—á–µ–Ω–∏—è expert vectors."""
    
    rl_reward_type: str = "task_accuracy"
    """–¢–∏–ø reward: 'task_accuracy', 'perplexity', 'custom'."""
    
    # ============================================================================
    # OVERRIDE –ë–ê–ó–û–í–´–• –ó–ù–ê–ß–ï–ù–ò–ô
    # ============================================================================
    
    experiment_name: str = "transformer_squared_bigobench_training"
    model_name: str = "transformer_squared_model"
    learning_rate: float = 2e-4
    weight_decay: float = 0.02

    # ============================================================================
    # COMPLEXITY PREDICTION (–¥–ª—è BigOBench –∑–∞–¥–∞—á–∏)
    # ============================================================================

    enable_complexity_head: bool = False 
    """–î–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π head –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è time/space complexity.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è multi-task learning –Ω–∞ BigOBench –¥–∞—Ç–∞—Å–µ—Ç–µ."""

    num_complexity_classes: int = 10
    """–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä: O(1), O(log n), O(n), O(n log n), ...).
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ enable_complexity_head=True."""

    complexity_head_hidden_dim: int = 256
    """–†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è –≤ complexity prediction head."""
    
    # ============================================================================
    # –ú–ï–¢–û–î–´
    # ============================================================================
    
    def __post_init__(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è."""
        
        # –í—ã–∑—ã–≤–∞–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π __post_init__
        super().__post_init__()
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º n_inner
        if self.n_inner is None:
            self.n_inner = 4 * self.n_embd
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è: n_embd –∫—Ä–∞—Ç–Ω–æ n_head
        if self.n_embd % self.n_head != 0:
            raise ValueError(
                f"n_embd ({self.n_embd}) –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∫—Ä–∞—Ç–Ω–æ n_head ({self.n_head})"
            )
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è: expert_dim —Ä–∞–∑—É–º–Ω—ã–π
        if self.expert_dim > self.n_embd:
            import warnings
            warnings.warn(
                f"expert_dim ({self.expert_dim}) > n_embd ({self.n_embd}). "
                f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: expert_dim <= n_embd/2"
            )
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è: apply_svf_to
        valid_targets = {'q', 'k', 'v', 'o', 'ffn'}
        for target in self.apply_svf_to:
            if target not in valid_targets:
                raise ValueError(
                    f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è SVF target: {target}. –î–æ–ø—É—Å—Ç–∏–º—ã–µ: {valid_targets}"
                )
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è: adaptive span
        if self.adaptive_span_enabled:
            if self.max_adaptive_span > self.block_size:
                raise ValueError(
                    f"max_adaptive_span ({self.max_adaptive_span}) > "
                    f"block_size ({self.block_size})"
                )
            if self.min_adaptive_span >= self.max_adaptive_span:
                raise ValueError(
                    "min_adaptive_span –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å < max_adaptive_span"
                )
        
        # –ï—Å–ª–∏ bias –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ –∑–∞–¥–∞–Ω—ã —è–≤–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º use_bias
        if not hasattr(self, '_bias_set'):
            if self.attention_bias is None:
                self.attention_bias = self.use_bias
            if self.mlp_bias is None:
                self.mlp_bias = self.use_bias
        
        # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.model_type = 'transformer_squared'
        self.num_expert_vectors = self.num_experts
        
        # –û—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        self._estimate_parameters()
    
    def _estimate_parameters(self):
        """–û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."""
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        token_emb = self.vocab_size * self.n_embd
        pos_emb = 0 if self.use_rope else self.block_size * self.n_embd
        
        base_layer_params = (
            2 * self.n_embd +                  # LN 1
            4 * self.n_embd * self.n_embd +    # Attention
            2 * self.n_embd +                  # LN 2
            2 * self.n_embd * self.n_inner     # FFN
        )
        
        # SVF –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        svf_params_per_layer = 0
        if self.use_svf:
            num_svf_matrices = len(self.apply_svf_to)
            # SVF –∏—Å–ø–æ–ª—å–∑—É–µ—Ç low-rank decomposition
            avg_dim = self.n_embd
            svf_params_per_layer = num_svf_matrices * (avg_dim * self.svf_rank * 2)
        
        # Expert parameters
        expert_params = self.num_experts * self.expert_dim
        
        # Classifier
        classifier_params = 0
        if self.adaptation_strategy == 'classifier':
            classifier_params = self.n_embd * self.num_experts
        
        final_ln = 2 * self.n_embd
        
        total_base = token_emb + pos_emb + (base_layer_params * self.n_layer) + final_ln
        total_svf = svf_params_per_layer * self.n_layer
        total_experts = expert_params + classifier_params
        
        total = total_base + total_svf + total_experts
        
        self.estimated_parameters = total
        self.estimated_parameters_millions = total / 1e6
        self.estimated_base_parameters = total_base
        self.estimated_svf_parameters = total_svf
        self.estimated_expert_parameters = total_experts
    
    def get_head_dim(self) -> int:
        """–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ attention head."""
        return self.n_embd // self.n_head
    
    def get_dtype(self) -> torch.dtype:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å—Ç—Ä–æ–∫–∏ dtype –≤ torch.dtype."""
        dtype_map = {
            'float32': torch.float32,
            'float16': torch.float16,
            'bfloat16': torch.bfloat16,
        }
        return dtype_map.get(self.model_dtype, torch.float32)
    
    def validate(self):
        """–Ø–≤–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏."""
        # –í—Å—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤ __post_init__, —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        pass
