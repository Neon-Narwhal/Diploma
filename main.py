"""–ì–ª–∞–≤–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –æ–±—É—á–µ–Ω–∏—è –∏ MLflow"""

import torch
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –ø–∞–ø–∫—É –≤ –ø—É—Ç—å Python
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# –ü—Ä—è–º—ã–µ –∏–º–ø–æ—Ä—Ç—ã –∏–∑ –ø–∞–ø–æ–∫ –≤ –∫–æ—Ä–Ω–µ
from models.gpt_model import GPTLikeModel
from utils.config import ModelConfig
from my_tokenizers.char_tokenizer import CharTokenizer
from my_tokenizers.bpe_tokenizer import BPETokenizer
from utils.logging import GenerationLogger
from utils.data_utils import load_data, prepare_data

from training.transformer_squared_training import run_quick_test, run_laptop_test
from training.trainer import ModelTrainer, AdvancedModelTrainer




def demo_basic_training():
    """üèãÔ∏è‚Äç‚ôÇÔ∏è –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ MLflow"""

    print("\n" + "="*60)
    print("üèãÔ∏è‚Äç‚ôÇÔ∏è –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ë–ê–ó–û–í–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
    print("="*60)

    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    config = ModelConfig(
        tokenizer_type='char',
        max_iters=50,          
        eval_interval=10,
        n_embd=32,             # –ú–∞–ª–µ–Ω—å–∫–∞—è –º–æ–¥–µ–ª—å
        n_layer=2,
        n_head=2,
        learning_rate=3e-4
    )

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    data = load_data(config.data_file)
    tokenizer = CharTokenizer(data)
    config.vocab_size = tokenizer.get_vocab_size()

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    data_tensor = prepare_data(data, tokenizer, config)


    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = GPTLikeModel(config).to(config.device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)

    print(f"\nüß† –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞:")
    print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(data_tensor):,} —Ç–æ–∫–µ–Ω–æ–≤")

    # –ü—Ä–æ—Å—Ç–æ–π —Ç—Ä–µ–Ω–µ—Ä
    trainer = ModelTrainer(model, optimizer, config)

    print("\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")

    # –û–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ—Å—Ç—ã–º callback
    def training_callback(step, metrics):
        if step % 20 == 0:
            print(f"  üìä –®–∞–≥ {step}: train={metrics.get('train', 0):.3f}, val={metrics.get('val', 0):.3f}")

    results = trainer.train_epoch(data_tensor, callback=training_callback)

    print(f"\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è:")
    final_losses = results.get('final_losses', {})
    print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–π train loss: {final_losses.get('train', 'N/A'):.4f}")
    print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–π val loss: {final_losses.get('val', 'N/A'):.4f}")

    if 'early_stopped_at' in results:
        print(f"   ‚èπÔ∏è Early stopping –Ω–∞ —à–∞–≥–µ: {results['early_stopped_at']}")

    print("‚úÖ –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

    return model, tokenizer, config


def demo_generation_after_training(model, tokenizer, config):
    """üé® –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è"""

    if model is None:
        print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è")
        return

    model.eval()
    context = torch.zeros((1, 1), dtype=torch.long, device=config.device)

    print("üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é (—Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º):")
    print("-" * 50)

    with torch.no_grad():
        generated = model.generate_with_logging(
            context, 
            max_new_token=80, 
            tokenizer=tokenizer,
            temperature=0.9
        )

    print("-" * 50)
    print("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")



if __name__ == "__main__":
    torch.cuda.empty_cache()
    #main()
    run_quick_test() ## sakana
    #run_laptop_test()