"""–ì–ª–∞–≤–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –æ–±—É—á–µ–Ω–∏—è –∏ MLflow"""

import torch
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –ø–∞–ø–∫—É –≤ –ø—É—Ç—å Python
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# –ü—Ä—è–º—ã–µ –∏–º–ø–æ—Ä—Ç—ã –∏–∑ –ø–∞–ø–æ–∫ –≤ –∫–æ—Ä–Ω–µ
from models.gpt_model import GPTLikeModel
from utils.config import ModelConfig
from my_tokenizers.char_tokenizer import CharTokenizer
from my_tokenizers.bpe_tokenizer import BPETokenizer
from utils.logging import GenerationLogger
from utils.data_utils import load_data, prepare_data

from training.transformer_squared_training import run_quick_test, run_laptop_test
from training.trainer import ModelTrainer, AdvancedModelTrainer
from experiments.mlflow_experiments import ExperimentRunner, run_single_experiment




def demo_basic_training():
    """üèãÔ∏è‚Äç‚ôÇÔ∏è –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ MLflow"""

    print("\n" + "="*60)
    print("üèãÔ∏è‚Äç‚ôÇÔ∏è –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ë–ê–ó–û–í–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
    print("="*60)

    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    config = ModelConfig(
        tokenizer_type='char',
        max_iters=50,          
        eval_interval=10,
        n_embd=32,             # –ú–∞–ª–µ–Ω—å–∫–∞—è –º–æ–¥–µ–ª—å
        n_layer=2,
        n_head=2,
        learning_rate=3e-4
    )

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    data = load_data(config.data_file)
    tokenizer = CharTokenizer(data)
    config.vocab_size = tokenizer.get_vocab_size()

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    data_tensor = prepare_data(data, tokenizer, config)


    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = GPTLikeModel(config).to(config.device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)

    print(f"\nüß† –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞:")
    print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(data_tensor):,} —Ç–æ–∫–µ–Ω–æ–≤")

    # –ü—Ä–æ—Å—Ç–æ–π —Ç—Ä–µ–Ω–µ—Ä
    trainer = ModelTrainer(model, optimizer, config)

    print("\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")

    # –û–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ—Å—Ç—ã–º callback
    def training_callback(step, metrics):
        if step % 20 == 0:
            print(f"  üìä –®–∞–≥ {step}: train={metrics.get('train', 0):.3f}, val={metrics.get('val', 0):.3f}")

    results = trainer.train_epoch(data_tensor, callback=training_callback)

    print(f"\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è:")
    final_losses = results.get('final_losses', {})
    print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–π train loss: {final_losses.get('train', 'N/A'):.4f}")
    print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–π val loss: {final_losses.get('val', 'N/A'):.4f}")

    if 'early_stopped_at' in results:
        print(f"   ‚èπÔ∏è Early stopping –Ω–∞ —à–∞–≥–µ: {results['early_stopped_at']}")

    print("‚úÖ –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

    return model, tokenizer, config


def demo_mlflow_training():
    """üî¨ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å MLflow –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""


    try:
        # –ó–∞–ø—É—Å–∫ –±—ã—Å—Ç—Ä–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ –≥–æ—Ç–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é
        print("üß™ –ó–∞–ø—É—Å–∫ –±—ã—Å—Ç—Ä–æ–≥–æ MLflow —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞...")

        final_loss = run_single_experiment(
            vocab_size=300,
            tokenizer_type='char',
            experiment_name="main_demo_experiments"
        )

        print(f"\n‚úÖ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–π validation loss: {final_loss:.4f}")
        print("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ MLflow")

        return None, None, None  # –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ MLflow

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ MLflow —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: {e}")
        return None, None, None


def demo_advanced_mlflow_training():
    """‚öôÔ∏è –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ MLflow —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""



    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π experiment runner
    runner = ExperimentRunner(experiment_name="test_rope_model")

    # –ö–∞—Å—Ç–æ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    config = ModelConfig(
        tokenizer_type='char',
        max_iters=1000,
        model_name="transformer_rope_model"
    )


    # –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ - —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
    def custom_model_complexity(model, data, config):
        return sum(p.numel() for p in model.parameters()) / 1000.0

    final_loss = runner.run_experiment(
        config=config,
        run_name="advanced_demo_run",
        generation_samples=3,      # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º 3 –ø—Ä–∏–º–µ—Ä–∞
        save_model=True,          # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        custom_metrics={'model_complexity_k': custom_model_complexity}
    )

    print(f"\nüéØ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–π loss: {final_loss:.4f}")
    print("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ MLflow")



def demo_generation_after_training(model, tokenizer, config):
    """üé® –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è"""

    if model is None:
        print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è")
        return

    model.eval()
    context = torch.zeros((1, 1), dtype=torch.long, device=config.device)

    print("üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é (—Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º):")
    print("-" * 50)

    with torch.no_grad():
        generated = model.generate_with_logging(
            context, 
            max_new_token=80, 
            tokenizer=tokenizer,
            temperature=0.9
        )

    print("-" * 50)
    print("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


def main():
    """üöÄ –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –≤—ã–±–æ—Ä–æ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–π"""

    torch.manual_seed(ModelConfig.seed)


    demo_advanced_mlflow_training()


if __name__ == "__main__":
    torch.cuda.empty_cache()
    #main()
    run_quick_test() ## sakana
    #run_laptop_test()