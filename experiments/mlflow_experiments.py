"""
üî¨ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–´ –° MLFLOW - –ü–û–õ–ù–´–ô –ì–ò–ë–ö–ò–ô –ö–û–î
==============================================

–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≥–∏–±–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –¥–ª—è GPT-–º–æ–¥–µ–ª–∏ —Å:
- MLflow tracking –¥–ª—è –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤  
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
- Early stopping –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏
- –ì–∏–±–∫–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
"""

import torch
import mlflow
import mlflow.pytorch
import numpy as np
from dataclasses import asdict
from typing import Dict, Any, List, Optional, Callable, Union
import json
import tempfile
import os
from pathlib import Path

import sys
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)


from models.gpt_model import GPTLikeModel
from utils.config import ModelConfig
from utils.data_utils import load_data, get_batch, prepare_data
from utils.logging import GenerationLogger
from my_tokenizers.char_tokenizer import CharTokenizer
from training.trainer import ModelTrainer, estimate_loss


from my_tokenizers.bpe_tokenizer import BPETokenizer
HAS_BPE = True



class ExperimentRunner:
    """
    üéØ –ì–õ–ê–í–ù–´–ô –ö–õ–ê–°–° –î–õ–Ø –£–ü–†–ê–í–õ–ï–ù–ò–Ø –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê–ú–ò

    –ì–∏–±–∫–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–π runner –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å:
    - –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    - –ì–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """

    def __init__(self, 
                 experiment_name: str = "gpt_experiments",
                 tracking_uri: Optional[str] = None,
                 auto_log: bool = True):
        """
        Args:
            experiment_name: –ù–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –≤ MLflow
            tracking_uri: URI –¥–ª—è MLflow tracking server
            auto_log: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ PyTorch –º–µ—Ç—Ä–∏–∫
        """
        self.experiment_name = experiment_name
        self.logger = GenerationLogger()

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ MLflow
        if tracking_uri:
            mlflow.set_tracking_uri(tracking_uri)

        mlflow.set_experiment(experiment_name)

        if auto_log:
            mlflow.pytorch.autolog(log_models=False)  # –û—Ç–∫–ª—é—á–∞–µ–º –∞–≤—Ç–æ–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è

    def create_tokenizer(self, data: str, config: ModelConfig):
        """
        üî§ –£–ú–ù–û–ï –°–û–ó–î–ê–ù–ò–ï –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê

        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        """
        if config.tokenizer_type == 'char':
            return CharTokenizer(data)
        elif config.tokenizer_type == 'bpe' :
            return BPETokenizer(data, config.vocab_size)

        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {config.tokenizer_type}")

    def run_experiment(self, 
                      config: ModelConfig, 
                      run_name: Optional[str] = None,
                      generation_samples: int = 3,
                      save_model: bool = True,
                      custom_metrics: Optional[Dict[str, Callable]] = None) -> float:
        """
        üöÄ –ó–ê–ü–£–°–ö –û–î–ù–û–ì–û –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê

        –ü–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ —Å MLflow tracking

        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
            run_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞
            generation_samples: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            save_model: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –º–æ–¥–µ–ª—å –≤ MLflow
            custom_metrics: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

        Returns:
            float: –§–∏–Ω–∞–ª—å–Ω—ã–π validation loss
        """

        # üìä –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•
        self.logger.logger.info(f"üöÄ –ù–∞—á–∞–ª–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: {run_name or 'Unnamed'}")

        data = load_data(config.data_file)
        tokenizer = self.create_tokenizer(data, config)
        actual_vocab_size = tokenizer.get_vocab_size()

        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å —Ä–µ–∞–ª—å–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º —Å–ª–æ–≤–∞—Ä—è
        config.vocab_size = actual_vocab_size

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        data_torch = prepare_data(data, tokenizer, config)

        # üéØ MLflow RUN
        with mlflow.start_run(run_name=run_name):

            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            params_dict = asdict(config)
            params_dict.update({
                "actual_vocab_size": actual_vocab_size,
                "data_length": len(data_torch),
                "data_chars": len(data),
                "tokenizer_efficiency": len(data_torch) / len(data)  # —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —Å–∏–º–≤–æ–ª
            })
            mlflow.log_params(params_dict)

            # üß† –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò
            model = GPTLikeModel(config)
            model = model.to(config.device)

            param_count = sum(p.numel() for p in model.parameters())
            model_size_mb = param_count * 4 / (1024 * 1024)  # –ø—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≤ MB

            mlflow.log_params({
                "model_parameters": param_count,
                "model_size_mb": round(model_size_mb, 2)
            })

            self.logger.logger.info(f"üìä –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞: {param_count:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ({model_size_mb:.1f}MB)")

            # üèÉ‚Äç‚ôÇÔ∏è –û–ë–£–ß–ï–ù–ò–ï
            optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)
            trainer = ModelTrainer(model, optimizer, config)

            # Callback –¥–ª—è MLflow –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            def mlflow_callback(iter_num: int, losses: Dict[str, float]):
                metrics = {
                    "train_loss": losses['train'],
                    "val_loss": losses['val'],
                    "loss_diff": losses['val'] - losses['train'],  # overfit indicator
                    "learning_rate": optimizer.param_groups[0]['lr']
                }

                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
                if custom_metrics:
                    for name, metric_func in custom_metrics.items():
                        try:
                            value = metric_func(model, data_torch, config)
                            metrics[f"custom_{name}"] = value
                        except Exception as e:
                            self.logger.logger.warning(f"–û—à–∏–±–∫–∞ –≤ –º–µ—Ç—Ä–∏–∫–µ {name}: {e}")

                mlflow.log_metrics(metrics, step=iter_num)

            # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
            training_results = trainer.train_epoch(data_torch, callback=mlflow_callback)

            # üìà –§–ò–ù–ê–õ–¨–ù–´–ï –ú–ï–¢–†–ò–ö–ò
            final_losses = training_results['final_losses']

            final_metrics = {
                "final_train_loss": final_losses['train'],
                "final_val_loss": final_losses['val'],
                "final_overfit": final_losses['val'] - final_losses['train'],
                "epochs_completed": 1,
                "training_efficiency": final_losses['train'] / training_results.get('epoch_losses', [1])[0] if training_results.get('epoch_losses') else 1.0
            }

            # Early stopping –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            if 'early_stopped_at' in training_results:
                final_metrics["early_stopped_at"] = training_results['early_stopped_at']
                final_metrics["training_completed"] = False
            else:
                final_metrics["training_completed"] = True

            mlflow.log_metrics(final_metrics)

            # üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò
            if save_model:
                try:
                    mlflow.pytorch.log_model(
                        pytorch_model=model,
                        artifact_path="model",
                        registered_model_name=config.model_name,
                        signature=None  # TODO: –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å input/output signature
                    )
                    self.logger.logger.info("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ MLflow")
                except Exception as e:
                    self.logger.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")

            # üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                tokenizer_path = f.name
                tokenizer.save(tokenizer_path)
                mlflow.log_artifact(tokenizer_path, "tokenizer")
                os.unlink(tokenizer_path)  # —É–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª

            # üé® –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–ë–†–ê–ó–¶–û–í –¢–ï–ö–°–¢–ê
            self._generate_and_log_samples(model, tokenizer, config, generation_samples)

            # üìä –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ê–ù–ê–õ–ò–¢–ò–ö–ê
            self._log_model_analysis(model, data_torch, config)

            self.logger.logger.info(f"‚úÖ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: val_loss = {final_losses['val']:.4f}")

            return final_losses['val']

    def _generate_and_log_samples(self, model, tokenizer, config, num_samples: int):
        """üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞–∑—Ü–æ–≤ —Ç–µ–∫—Å—Ç–∞"""

        model.eval()
        self.logger.logger.info(f"üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {num_samples} –æ–±—Ä–∞–∑—Ü–æ–≤ —Ç–µ–∫—Å—Ç–∞...")

        samples = {}

        for i in range(num_samples):
            context = torch.zeros((1, 1), dtype=torch.long, device=config.device)

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞
            if i == 0:
                with torch.no_grad():
                    generated = model.generate_with_logging(
                        context, 
                        max_new_token=100, 
                        tokenizer=tokenizer,
                        temperature=0.8
                    )
                    sample_text = tokenizer.decode(generated[0].tolist())
                    samples[f"sample_{i+1}_with_logging"] = sample_text
            else:
                # –û–±—ã—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
                with torch.no_grad():
                    generated = model.generate(context, max_new_token=100)
                    sample_text = tokenizer.decode(generated[0].tolist())
                    samples[f"sample_{i+1}"] = sample_text

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow
        for name, text in samples.items():
            mlflow.log_text(text, f"generated/{name}.txt")

        # –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        avg_length = np.mean([len(text) for text in samples.values()])
        unique_chars = len(set(''.join(samples.values())))

        mlflow.log_metrics({
            "generation_avg_length": avg_length,
            "generation_unique_chars": unique_chars,
            "generation_samples": num_samples
        })

    def _log_model_analysis(self, model, data_torch, config):
        """üìä –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏"""

        try:
            # –ê–Ω–∞–ª–∏–∑ –≤–µ—Å–æ–≤
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ—Å–æ–≤
            all_weights = []
            for param in model.parameters():
                if param.requires_grad:
                    all_weights.extend(param.data.cpu().numpy().flatten())

            weights_array = np.array(all_weights)

            analysis_metrics = {
                "model_total_params": total_params,
                "model_trainable_params": trainable_params,
                "weights_mean": float(np.mean(weights_array)),
                "weights_std": float(np.std(weights_array)),
                "weights_min": float(np.min(weights_array)),
                "weights_max": float(np.max(weights_array)),
                "data_vocab_coverage": len(torch.unique(data_torch)) / config.vocab_size
            }

            mlflow.log_metrics(analysis_metrics)

        except Exception as e:
            self.logger.logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –≤ –∞–Ω–∞–ª–∏–∑–µ –º–æ–¥–µ–ª–∏: {e}")

    def run_vocab_size_experiments(self,
                                  vocab_sizes: List[int] = None,
                                  tokenizer_types: List[str] = None,
                                  base_config: ModelConfig = None) -> List[Dict[str, Any]]:
        """
        üìä –ó–ê–ü–£–°–ö –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–û–í –° –†–ê–ó–ù–´–ú–ò –†–ê–ó–ú–ï–†–ê–ú–ò –°–õ–û–í–ê–†–Ø

        Args:
            vocab_sizes: –†–∞–∑–º–µ—Ä—ã —Å–ª–æ–≤–∞—Ä–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            tokenizer_types: –¢–∏–ø—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
            base_config: –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–±—É–¥–µ—Ç —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∞ –∏ –∏–∑–º–µ–Ω–µ–Ω–∞)

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
        """

        if vocab_sizes is None:
            vocab_sizes = [100, 200, 500, 1000]

        if tokenizer_types is None:
            tokenizer_types = ['char'] + (['bpe'] if HAS_BPE else [])

        if base_config is None:
            base_config = ModelConfig(
                max_iters=500,
                eval_interval=100,
                n_embd=128,
                n_layer=3,
                n_head=4
            )

        results = []
        total_experiments = len(vocab_sizes) * len(tokenizer_types)
        current_experiment = 0

        self.logger.logger.info(f"üî¨ –ó–∞–ø—É—Å–∫ {total_experiments} —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤...")

        for tokenizer_type in tokenizer_types:
            for vocab_size in vocab_sizes:
                current_experiment += 1

                # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
                config = ModelConfig(
                    vocab_size=vocab_size,
                    tokenizer_type=tokenizer_type,
                    max_iters=base_config.max_iters,
                    eval_interval=base_config.eval_interval,
                    n_embd=base_config.n_embd,
                    n_layer=base_config.n_layer,
                    n_head=base_config.n_head,
                    learning_rate=base_config.learning_rate,
                    model_name=f"gpt_model_{tokenizer_type}_{vocab_size}"
                )

                run_name = f"{tokenizer_type}_vocab_{vocab_size}"

                self.logger.logger.info(f"\nüöÄ [{current_experiment}/{total_experiments}] –ó–∞–ø—É—Å–∫: {run_name}")

                try:
                    final_val_loss = self.run_experiment(
                        config, 
                        run_name=run_name,
                        generation_samples=2,  # –º–µ–Ω—å—à–µ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                        save_model=True
                    )

                    result = {
                        'tokenizer_type': tokenizer_type,
                        'vocab_size': vocab_size,
                        'final_val_loss': final_val_loss,
                        'model_name': config.model_name,
                        'success': True
                    }

                    results.append(result)
                    self.logger.logger.info(f"‚úÖ [{current_experiment}/{total_experiments}] –ó–∞–≤–µ—Ä—à–µ–Ω {run_name}: val_loss = {final_val_loss:.4f}")

                except Exception as e:
                    self.logger.logger.error(f"‚ùå [{current_experiment}/{total_experiments}] –û—à–∏–±–∫–∞ –≤ {run_name}: {e}")
                    results.append({
                        'tokenizer_type': tokenizer_type,
                        'vocab_size': vocab_size,
                        'final_val_loss': float('inf'),
                        'model_name': config.model_name,
                        'success': False,
                        'error': str(e)
                    })

        # –°–≤–æ–¥–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self._print_experiment_summary(results)
        self._log_experiment_summary(results)

        return results

    def _print_experiment_summary(self, results: List[Dict[str, Any]]):
        """üìä –ü–µ—á–∞—Ç—å —Å–≤–æ–¥–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""

        print("\n" + "="*80)
        print("üìä –°–í–û–î–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–û–í")
        print("="*80)

        successful_results = [r for r in results if r['success']]
        failed_results = [r for r in results if not r['success']]

        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {len(successful_results)}/{len(results)}")
        print(f"‚ùå –û—à–∏–±–∫–∏: {len(failed_results)}/{len(results)}")

        if successful_results:
            print("\nüèÜ –õ–£–ß–®–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
            sorted_results = sorted(successful_results, key=lambda x: x['final_val_loss'])

            for i, result in enumerate(sorted_results[:5], 1):
                print(f"  {i}. {result['tokenizer_type'].upper()} vocab={result['vocab_size']:4d}: "
                      f"val_loss={result['final_val_loss']:.4f}")

        if failed_results:
            print("\n‚ùå –ù–ï–£–î–ê–ß–ù–´–ï –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–´:")
            for result in failed_results:
                print(f"  ‚Ä¢ {result['tokenizer_type'].upper()} vocab={result['vocab_size']:4d}: {result['error']}")

        print("="*80)

    def _log_experiment_summary(self, results: List[Dict[str, Any]]):
        """üìà –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –≤ MLflow"""

        try:
            # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π run –¥–ª—è —Å–≤–æ–¥–∫–∏
            with mlflow.start_run(run_name="experiment_summary"):
                successful_results = [r for r in results if r['success']]

                if successful_results:
                    losses = [r['final_val_loss'] for r in successful_results]
                    best_result = min(successful_results, key=lambda x: x['final_val_loss'])

                    summary_metrics = {
                        "summary_total_experiments": len(results),
                        "summary_successful_experiments": len(successful_results),
                        "summary_failed_experiments": len(results) - len(successful_results),
                        "summary_best_val_loss": best_result['final_val_loss'],
                        "summary_worst_val_loss": max(losses),
                        "summary_mean_val_loss": np.mean(losses),
                        "summary_std_val_loss": np.std(losses)
                    }

                    mlflow.log_metrics(summary_metrics)

                    # –õ–æ–≥–∏—Ä—É–µ–º –ª—É—á—à—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
                    mlflow.log_params({
                        "best_tokenizer": best_result['tokenizer_type'],
                        "best_vocab_size": best_result['vocab_size'],
                        "best_model_name": best_result['model_name']
                    })

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∫ JSON
                    results_json = json.dumps(results, indent=2, ensure_ascii=False)
                    mlflow.log_text(results_json, "detailed_results.json")

        except Exception as e:
            self.logger.logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å–≤–æ–¥–∫–∏: {e}")


def run_single_experiment(vocab_size: int = 1000, 
                         tokenizer_type: str = 'char',
                         experiment_name: str = "single_test_runs") -> float:
    """
    üß™ –ë–´–°–¢–†–´–ô –ó–ê–ü–£–°–ö –û–î–ù–û–ì–û –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê

    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–¥–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    """

    runner = ExperimentRunner(experiment_name=experiment_name)

    config = ModelConfig(
        vocab_size=vocab_size,
        tokenizer_type=tokenizer_type,
        max_iters=200,
        eval_interval=50,
        n_embd=64,
        n_layer=2,
        n_head=2,
        model_name=f"test_model_{tokenizer_type}_{vocab_size}"
    )

    run_name = f"test_{tokenizer_type}_vocab_{vocab_size}"

    print(f"üß™ –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç: {run_name}")
    final_val_loss = runner.run_experiment(config, run_name, generation_samples=1)

    print(f"üéØ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: val_loss = {final_val_loss:.4f}")
    return final_val_loss


# üöÄ –ì–õ–ê–í–ù–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò –° –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ú –ö–û–î–û–ú
def run_experiment(config: ModelConfig, run_name: str = None) -> float:
    """–ó–∞–ø—É—Å–∫ –æ–¥–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ - —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º API"""
    runner = ExperimentRunner()
    return runner.run_experiment(config, run_name)


def run_vocab_size_experiments() -> List[Dict[str, Any]]:
    """–ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ —Å–ª–æ–≤–∞—Ä—è - —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å"""
    runner = ExperimentRunner()
    return runner.run_vocab_size_experiments()


def create_tokenizer(data: str, config: ModelConfig):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ - —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å"""
    runner = ExperimentRunner()
    return runner.create_tokenizer(data, config)


if __name__ == "__main__":
    # üéØ –ü–†–ò–ú–ï–†–´ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø

    print("üöÄ DIPLOMA PROJECT - MLFLOW EXPERIMENTS")
    print("="*60)
    print("–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ–∂–∏–º—ã:")
    print("1. üß™ –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç")
    print("2. üî¨ –ü–æ–ª–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã")
    print("3. ‚öôÔ∏è  –ö–∞—Å—Ç–æ–º–Ω—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç")

    mode = input("\n–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º (1-3) –∏–ª–∏ Enter –¥–ª—è —Ç–µ—Å—Ç–∞: ").strip()

    if mode == "1" or mode == "":
        # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
        test_loss = run_single_experiment(vocab_size=500, tokenizer_type='char')

    elif mode == "2":
        # –ü–æ–ª–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
        runner = ExperimentRunner(experiment_name="vocab_size_comparison")
        results = runner.run_vocab_size_experiments()
        print(f"\nüéâ –ó–∞–≤–µ—Ä—à–µ–Ω–æ {len(results)} —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤!")

    elif mode == "3":
        # –ö–∞—Å—Ç–æ–º–Ω—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
        runner = ExperimentRunner(experiment_name="custom_experiments")

        config = ModelConfig(
            vocab_size=1000,
            tokenizer_type='char',
            max_iters=1000,
            eval_interval=200,
            n_embd=256,
            n_layer=6,
            n_head=4,
            model_name="custom_gpt_model"
        )

        final_loss = runner.run_experiment(
            config, 
            run_name="custom_experiment",
            generation_samples=5,
            save_model=True
        )

        print(f"\nüéØ –ö–∞—Å—Ç–æ–º–Ω—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: val_loss = {final_loss:.4f}")

    print("\nüéâ –í—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")
    print("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ—Å—Ç—É–ø–Ω—ã –≤ MLflow UI: mlflow ui")
