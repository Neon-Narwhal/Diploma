# =============================================================================
# experiments/train.py
# –ï–î–ò–ù–°–¢–í–ï–ù–ù–´–ô —Ñ–∞–π–ª –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –í–°–ï–• –º–æ–¥–µ–ª–µ–π –Ω–∞ BigOBench
# =============================================================================

"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ BigOBench.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏:
- GPT (GPTLikeModel)
- TransformerSquared (Transformer¬≤)
- –õ—é–±—ã–µ –±—É–¥—É—â–∏–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Registry

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    # 1. –û–±—É—á–µ–Ω–∏–µ GPT —Å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    python experiments/train.py
    
    # 2. –û–±—É—á–µ–Ω–∏–µ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (—Å–º. –ø—Ä–∏–º–µ—Ä—ã –≤–Ω–∏–∑—É —Ñ–∞–π–ª–∞)
    # –ò–∑–º–µ–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ TRAINING_CONFIG –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    - –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥–∏ (utils/configs/)
    - –ï–¥–∏–Ω—ã–π UniversalTrainer –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    - Registry –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
    - –ü–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å MLflow
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
"""

import sys
import os
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import json
import torch
from typing import Optional, Dict, Any

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
from utils.configs import GPTConfig, TransformerSquaredConfig

# –ú–æ–¥–µ–ª–∏
from models.gpt_model import GPTLikeModel

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã
from my_tokenizers.bpe_tokenizer import BPETokenizer
from my_tokenizers.char_tokenizer import CharTokenizer

# Training
from training.bigobench_dataset import create_dataloaders_from_config
from training.universal_trainer import UniversalTrainer
from training.model_registry import ModelRegistry

# –£—Ç–∏–ª–∏—Ç—ã
from utils.logging import setup_logging, MetricsFormatter

import logging


# =============================================================================
# –ù–ê–°–¢–†–û–ô–ö–ê –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø
# =============================================================================

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –±–∞–∑–æ–≤–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
setup_logging(log_level='INFO', log_file='logs/training.log')
logger = logging.getLogger(__name__)

# Formatter –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞ –º–µ—Ç—Ä–∏–∫
formatter = MetricsFormatter()


# =============================================================================
# –†–ï–ì–ò–°–¢–†–ê–¶–ò–Ø –ú–û–î–ï–õ–ï–ô
# =============================================================================

def register_all_models():
    """
    –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ Registry.
    
    –î–æ–±–∞–≤—å—Ç–µ —Å—é–¥–∞ –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ –º–µ—Ä–µ –∏—Ö —Å–æ–∑–¥–∞–Ω–∏—è.
    """
    logger.info("="*80)
    logger.info("–†–ï–ì–ò–°–¢–†–ê–¶–ò–Ø –ú–û–î–ï–õ–ï–ô")
    logger.info("="*80)
    
    # 1. –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º GPT
    ModelRegistry.register_model(
        name='gpt',
        model_class=GPTLikeModel,
        config_class=GPTConfig
    )
    logger.info("‚úÖ –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: GPT")
    
    # 2. –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º TransformerSquared (–µ—Å–ª–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω)
    try:
        from models.transformer_squared_model import TransformerSquared
        ModelRegistry.register_model(
            name='transformer_squared',
            model_class=TransformerSquared,
            config_class=TransformerSquaredConfig
        )
        logger.info("‚úÖ –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: TransformerSquared")
    except ImportError:
        logger.warning("‚ö†Ô∏è  TransformerSquared –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ GPT")
    
    # 3. –î–æ–±–∞–≤—å—Ç–µ —Å—é–¥–∞ –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏:
    # try:
    #     from models.my_new_model import MyNewModel
    #     from utils.configs.my_new_config import MyNewConfig
    #     ModelRegistry.register_model('my_new_model', MyNewModel, MyNewConfig)
    # except ImportError:
    #     pass
    
    logger.info(f"üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {ModelRegistry.list_models()}")
    logger.info("="*80 + "\n")


# =============================================================================
# –°–û–ó–î–ê–ù–ò–ï –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê
# =============================================================================

def create_tokenizer_from_file(
    data_path: str,
    tokenizer_type: str,
    vocab_size: int,
    max_samples: int = 1000
) -> tuple:
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞."""
    logger.info(f"üìö –°–æ–∑–¥–∞–Ω–∏–µ {tokenizer_type.upper()} —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
    data_path = Path(data_path)
    if not data_path.exists():
        logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_path}")
        logger.error(f"   –¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {Path.cwd()}")
        logger.error(f"   –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å: {data_path.absolute()}")
        
        # –ü–æ–¥—Å–∫–∞–∑–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
        logger.info("\nüí° –í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è:")
        logger.info("   1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        logger.info("   2. –ò–∑–º–µ–Ω–∏—Ç–µ data_path –≤ TRAINING_CONFIG")
        logger.info("   3. –°–∫–∞—á–∞–π—Ç–µ BigOBench –¥–∞—Ç–∞—Å–µ—Ç:")
        logger.info("      https://github.com/facebookresearch/BigOBench")
        
        raise FileNotFoundError(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_path}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
    texts = []
    
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= max_samples:
                    break
                
                if line.strip():
                    try:
                        data = json.loads(line)
                        code = data.get('query_code', '') or data.get('query_dataclass_code', '')
                        if code and len(code.strip()) > 10:
                            texts.append(code)
                    except json.JSONDecodeError as e:
                        logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω–∞ —Å—Ç—Ä–æ–∫–∞ {i}: –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON")
                        continue
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        raise
    
    if not texts:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –∏–∑ {data_path}")
        logger.error(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {i+1}, –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: 0")
        logger.info("\nüí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ:")
        logger.info("   1. –§–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å JSONL (JSON Lines)")
        logger.info("   2. –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø–æ–ª–µ 'query_code' –∏–ª–∏ 'query_dataclass_code'")
        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ {data_path}")
    
    logger.info(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–æ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞")
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç—ã
    combined_text = '\n'.join(texts)
    
    # –°–æ–∑–¥–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    if tokenizer_type == 'bpe':
        tokenizer = BPETokenizer(combined_text, vocab_size)
    elif tokenizer_type == 'char':
        tokenizer = CharTokenizer(combined_text)
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {tokenizer_type}")
    
    actual_vocab_size = tokenizer.get_vocab_size()
    logger.info(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω: vocab_size={actual_vocab_size}")
    
    return tokenizer, actual_vocab_size



# =============================================================================
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –û–ë–£–ß–ï–ù–ò–Ø
# =============================================================================

def train_model(
    model_name: str,
    config_overrides: Optional[Dict[str, Any]] = None,
    force_retrain: bool = False
) -> Dict[str, float]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –ª—é–±–æ–π –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.
    
    Args:
        model_name: –ò–º—è –º–æ–¥–µ–ª–∏ –∏–∑ registry ('gpt', 'transformer_squared', –∏ —Ç.–¥.)
        config_overrides: –°–ª–æ–≤–∞—Ä—å —Å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–æ–Ω—Ñ–∏–≥–∞
        force_retrain: –ï—Å–ª–∏ True, –æ–±—É—á–∞–µ—Ç –¥–∞–∂–µ –µ—Å–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ñ–∏–Ω–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
    
    Example:
        >>> metrics = train_model('gpt', {'batch_size': 16, 'learning_rate': 5e-4})
        >>> print(f"Best val loss: {metrics['best_val_loss']:.4f}")
    """
    logger.info("\n" + "="*80)
    logger.info(f"üöÄ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò: {model_name.upper()}")
    logger.info("="*80)
    
    # ========================================================================
    # 1. –ü–†–û–í–ï–†–ö–ê –î–û–°–¢–£–ü–ù–û–°–¢–ò –ú–û–î–ï–õ–ò
    # ========================================================================
    
    if model_name not in ModelRegistry.list_models():
        available = ', '.join(ModelRegistry.list_models())
        raise ValueError(
            f"–ú–æ–¥–µ–ª—å '{model_name}' –Ω–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞. "
            f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {available}"
        )
    
    # ========================================================================
    # 2. –°–û–ó–î–ê–ù–ò–ï –ö–û–ù–§–ò–ì–ê
    # ========================================================================
    
    logger.info(f"‚öôÔ∏è  –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è {model_name}...")
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–ª–∞—Å—Å –∫–æ–Ω—Ñ–∏–≥–∞ –∏–∑ registry
    ConfigClass = ModelRegistry.get_config_class(model_name)
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ —Å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
    config = ConfigClass()
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if config_overrides:
        logger.info(f"   –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π: {config_overrides}")
        config.update(**config_overrides)
    
    # –í—ã–≤–æ–¥–∏–º –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    logger.info(f"\nüìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è:")
    logger.info(f"   ‚Ä¢ –ú–æ–¥–µ–ª—å: {model_name}")
    logger.info(f"   ‚Ä¢ –î–∞–Ω–Ω—ã–µ: {config.data_path}")
    logger.info(f"   ‚Ä¢ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {config.tokenizer_type} (vocab={config.vocab_size})")
    logger.info(f"   ‚Ä¢ Batch size: {config.batch_size} x {config.gradient_accumulation_steps} = {config.effective_batch_size}")
    logger.info(f"   ‚Ä¢ Learning rate: {config.learning_rate:.2e}")
    logger.info(f"   ‚Ä¢ Max epochs: {config.max_epochs}")
    logger.info(f"   ‚Ä¢ Device: {config.device}")
    
    if hasattr(config, 'n_embd'):
        logger.info(f"   ‚Ä¢ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: n_embd={config.n_embd}, n_layer={config.n_layer}, n_head={config.n_head}")
    
    # ========================================================================
    # 3. –°–û–ó–î–ê–ù–ò–ï –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê
    # ========================================================================
    
    logger.info(f"\nüìö –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    
    tokenizer, actual_vocab_size = create_tokenizer_from_file(
        data_path=str(config.data_path),
        tokenizer_type=config.tokenizer_type,
        vocab_size=config.vocab_size,
        max_samples=1000
    )
    
    # –û–±–Ω–æ–≤–ª—è–µ–º vocab_size –≤ –∫–æ–Ω—Ñ–∏–≥–µ
    config.vocab_size = actual_vocab_size
    logger.info(f"   ‚Ä¢ Vocab size –æ–±–Ω–æ–≤–ª–µ–Ω: {actual_vocab_size}")
    
    # ========================================================================
    # 4. –°–û–ó–î–ê–ù–ò–ï DATALOADERS
    # ========================================================================
    
    logger.info(f"\nüì¶ –°–æ–∑–¥–∞–Ω–∏–µ DataLoaders...")
    
    try:
        train_loader, val_loader = create_dataloaders_from_config(
            data_path=str(config.data_path),
            tokenizer=tokenizer,
            config=config,
            train_split=config.train_split,
            num_workers=config.num_workers,
            filter_missing_complexity=config.filter_missing_complexity
        )
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è DataLoaders: {e}")
        raise
    
    logger.info(f"   ‚Ä¢ Train batches: {len(train_loader)}")
    logger.info(f"   ‚Ä¢ Val batches: {len(val_loader)}")
    logger.info(f"   ‚Ä¢ Total train samples: {len(train_loader.dataset)}")
    logger.info(f"   ‚Ä¢ Total val samples: {len(val_loader.dataset)}")
    
    # ========================================================================
    # 5. –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò
    # ========================================================================
    
    logger.info(f"\nüß† –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model_name}...")
    
    try:
        model = ModelRegistry.create_model(model_name, config)
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
        raise
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    logger.info(f"   ‚Ä¢ –ö–ª–∞—Å—Å –º–æ–¥–µ–ª–∏: {model.__class__.__name__}")
    logger.info(f"   ‚Ä¢ –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,} (~{total_params/1e6:.2f}M)")
    logger.info(f"   ‚Ä¢ –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,} (~{trainable_params/1e6:.2f}M)")
    
    if hasattr(config, 'estimated_parameters_millions'):
        logger.info(f"   ‚Ä¢ –û—Ü–µ–Ω–∫–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞: {config.estimated_parameters_millions:.2f}M")
    
    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    model = model.to(config.device)
    
    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –∫–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ (PyTorch 2.0+)
    if config.compile_model:
        logger.info("   ‚Ä¢ –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å torch.compile...")
        try:
            model = torch.compile(model)
            logger.info("   ‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞")
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏: {e}")
    
    # ========================================================================
    # 6. –°–û–ó–î–ê–ù–ò–ï –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–û–ì–û –¢–†–ï–ù–ï–†–ê
    # ========================================================================
    
    logger.info(f"\nüèãÔ∏è  –°–æ–∑–¥–∞–Ω–∏–µ UniversalTrainer...")
    
    try:
        trainer = UniversalTrainer(
            model=model,
            tokenizer=tokenizer,
            train_loader=train_loader,
            val_loader=val_loader,
            config=config
        )
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞: {e}")
        raise
    
    logger.info("   ‚úÖ UniversalTrainer –≥–æ—Ç–æ–≤ –∫ –æ–±—É—á–µ–Ω–∏—é")
    
    # ========================================================================
    # 7. –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø
    # ========================================================================
    
    logger.info("\n" + "="*80)
    logger.info("üéì –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø")
    logger.info("="*80 + "\n")
    
    try:
        final_metrics = trainer.train()
    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è  –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (Ctrl+C)")
        logger.info("   –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è...")
        # –¢—Ä–µ–Ω–µ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
        return {'interrupted': True}
    except Exception as e:
        logger.error(f"\n‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()
        raise
    
    # ========================================================================
    # 8. –í–´–í–û–î –§–ò–ù–ê–õ–¨–ù–´–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
    # ========================================================================
    
    logger.info("\n" + "="*80)
    logger.info("üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    logger.info("="*80)
    logger.info(f"   ‚Ä¢ Best validation loss: {final_metrics['best_val_loss']:.4f}")
    logger.info(f"   ‚Ä¢ Final epoch: {final_metrics['final_epoch']}")
    logger.info(f"   ‚Ä¢ Total training time: {final_metrics.get('total_time', 'N/A')}")
    logger.info(f"   ‚Ä¢ Model saved to: checkpoints/")
    logger.info("="*80 + "\n")
    
    return final_metrics


# =============================================================================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# =============================================================================

def compare_configs(config_variations: list) -> Dict[str, Any]:
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π.
    
    –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ª—É—á—à–∏—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    
    Args:
        config_variations: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏ –∫–æ–Ω—Ñ–∏–≥–∞
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
    
    Example:
        >>> variations = [
        ...     {'n_embd': 128, 'n_layer': 4},
        ...     {'n_embd': 256, 'n_layer': 6},
        ...     {'n_embd': 512, 'n_layer': 8},
        ... ]
        >>> results = compare_configs(variations)
    """
    logger.info("\n" + "="*80)
    logger.info("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ô")
    logger.info("="*80)
    
    results = []
    
    for i, config_override in enumerate(config_variations, 1):
        logger.info(f"\n{'='*80}")
        logger.info(f"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç {i}/{len(config_variations)}: {config_override}")
        logger.info(f"{'='*80}")
        
        try:
            metrics = train_model('gpt', config_overrides=config_override)
            results.append({
                'config': config_override,
                'metrics': metrics
            })
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ {i}: {e}")
            results.append({
                'config': config_override,
                'error': str(e)
            })
    
    # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É
    logger.info("\n" + "="*80)
    logger.info("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–†–ê–í–ù–ï–ù–ò–Ø")
    logger.info("="*80)
    
    for i, result in enumerate(results, 1):
        config_str = str(result['config'])
        if 'error' in result:
            logger.info(f"{i}. {config_str}: ERROR - {result['error']}")
        else:
            val_loss = result['metrics']['best_val_loss']
            logger.info(f"{i}. {config_str}: Val Loss = {val_loss:.4f}")
    
    logger.info("="*80 + "\n")
    
    return results


# =============================================================================
# –ì–õ–ê–í–ù–ê–Ø –¢–û–ß–ö–ê –í–•–û–î–ê
# =============================================================================

if __name__ == "__main__":
    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏
    register_all_models()
    
    # ========================================================================
    # –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –û–ë–£–ß–ï–ù–ò–Ø (–ò–ó–ú–ï–ù–ò–¢–ï –ü–û–î –°–í–û–ò –ù–£–ñ–î–´)
    # ========================================================================
    
    # –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    MODEL_NAME = 'transformer_squared'  # 'gpt' –∏–ª–∏ 'transformer_squared'
    
    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    TRAINING_CONFIG = {
        # –î–∞–Ω–Ω—ã–µ
        'data_path': 'data/complexity_labels_full_0-49.jsonl',
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        'tokenizer_type': 'char',  # 'bpe' –∏–ª–∏ 'char'
        'vocab_size': 5000,
        
        # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (–¥–ª—è GPT/T¬≤)
        'n_embd': 256,
        'n_layer': 6,
        'n_head': 4,
        
        # –û–±—É—á–µ–Ω–∏–µ
        'batch_size': 8,
        'learning_rate': 3e-4,
        'max_epochs': 1,
        'block_size': 512,
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        'gradient_accumulation_steps': 4,
        'use_amp': True,
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        'save_every_epochs': 2,
        'early_stopping_patience': 5,
        
        # MLflow
        'experiment_name': f'{MODEL_NAME}_bigobench_training',
    }
    
    # ========================================================================
    # –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø
    # ========================================================================
    
    # –í–∞—Ä–∏–∞–Ω—Ç 1: –û–±—É—á–µ–Ω–∏–µ —Å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏
    metrics = train_model(MODEL_NAME, config_overrides=TRAINING_CONFIG)
    
    # –í–∞—Ä–∏–∞–Ω—Ç 2: –û–±—É—á–µ–Ω–∏–µ —Å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    # metrics = train_model('gpt')
    
    # –í–∞—Ä–∏–∞–Ω—Ç 3: –û–±—É—á–µ–Ω–∏–µ TransformerSquared
    # metrics = train_model('transformer_squared')
    
    # –í–∞—Ä–∏–∞–Ω—Ç 4: –û–±—É—á–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º
    # metrics = train_model('gpt', config_overrides={'tokenizer_type': 'char'})
    
    # –í–∞—Ä–∏–∞–Ω—Ç 5: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
